{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Scoring - Full QP checks\n",
    "\n",
    "#### Business Rule:\n",
    "- All full QPs are not eligible to receive a score. \n",
    "- They must be in final score, with a neutral payment adjustment. \n",
    "\n",
    "*How can HIVVS tweak this test ?*\n",
    "- In your case, you may know the NPIs that are full QPs. So, tweak the query to accomodate your needs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializting the necessary data\n",
    "%reload_ext sparkmagic.magics\n",
    "!echo '{ \\\n",
    "\"wait_for_idle_timeout_seconds\": 60, \\\n",
    "\"livy_session_startup_timeout_seconds\": 180, \\\n",
    "\"ignore_ssl_errors\":true, \\\n",
    "\"custom_headers\":{\"X-Requested-By\":\"admin\"}, \\\n",
    "\"session_configs\":{ \\\n",
    " \"executorCores\":1,\"proxyUser\":\"XX\", \\\n",
    "\"jars\":[\"/final-scoring/jars/postgresql.jar\"]} \\\n",
    "}' > ~/.sparkmagic/config.json  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Provide your credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your username\n",
    "import getpass\n",
    "myuser='J5D6'\n",
    "mypass=getpass.getpass('Enter your A&R Internal LDAP password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup\n",
    "%spark add -s $myuser -l scala  -k -u https://ambari.impl.qppar.internal:8443/qpp-ar-impl-hadoop/default/livy/v1 -a $myuser -p $mypass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must wait for the above step to finish. <span style=\"color:red\">some **Do you see the yarn application id ?** text</span>\n",
    "\n",
    "Run the house keeping steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "val RUN_ID = \"24\"\n",
    "val DB_SERVERS = Map(\"uds\" -> \"uds.impl.qppar.internal\", \"deid\" -> \"uds-deidentify.impl.qppar.internal\")\n",
    "\n",
    "val credentials = sc.textFile(\"pgpass.txt\").toDF()\n",
    "  \n",
    "\n",
    "def loadFromUDS(sql:String) = {\n",
    "    val server = \"uds-deidentify.impl.qppar.internal\" \n",
    "    //DB_SERVERS(DB)\n",
    "    val credential = credentials.filter($\"value\".startsWith(\"uds\")).as[String].head.split(\",\")\n",
    "    val userName = credential(1).toLowerCase()\n",
    "    val password = credential(2)\n",
    "    \n",
    "\n",
    "    spark.read.format(\"jdbc\").\n",
    "    option(\"driver\", \"org.postgresql.Driver\").\n",
    "    option(\"url\", s\"jdbc:postgresql://$server:5432/uds?sslfactory=org.postgresql.ssl.NonValidatingFactory&ssl=true&sslmode=require\").\n",
    "    option(\"dbtable\", s\"($sql) as tab\").\n",
    "    option(\"user\", userName).\n",
    "    option(\"password\", password).\n",
    "    load()\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "import java.lang.Double\n",
    "\n",
    "\n",
    "case class FinalScoreVO(\n",
    "    tin: String,\n",
    "    npi: String,\n",
    "    apmEntityId: String = null,\n",
    "    apmId: String = null,\n",
    "    submissionId: String = null,\n",
    "    apmEntitySubmissionId: String = null,\n",
    "    overallScore: Double,\n",
    "    qualityScore: Double,\n",
    "    qualityWeight: Double,\n",
    "    qualityReason: String,\n",
    "    piScore: Double,\n",
    "    piWeight: Double,\n",
    "    piReason: String,\n",
    "    iaScore: Double,\n",
    "    iaWeight: Double,\n",
    "    iaReason: String,\n",
    "    costScore: Double,\n",
    "    costWeight: Double,\n",
    "    costReason: String,\n",
    "    complexPatientBonus: Double = null,\n",
    "    smallPracticeBonus: Double = null,\n",
    "    reasons: List[String] = null,\n",
    "    isFinal: Boolean,\n",
    "    isVoluntary: Boolean,\n",
    "    isEligible: Boolean,\n",
    "    scoreId: String = null\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the final score data from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o fs\n",
    "val fs = spark.sql(s\"SELECT * FROM final_scoring.finalscore_$RUN_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "print (s\" The total number of final score entries in run ${RUN_ID}: ${fs.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark fs.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the list of full QPs in UDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o qps \n",
    "val qps = loadFromUDS(\"\"\"\n",
    "    SELECT DISTINCT npi, qp_status FROM active.provider\n",
    "    WHERE run in (0, 3) AND year = 2018 AND ( qp_status = 'Y' OR qp_status = 'Q' )\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "qps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "val joined =  fs.join(qps, Seq(\"npi\"), \"inner\")\n",
    "joined.createOrReplaceTempView(\"joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "joined.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "val nonZeroScores = spark.sql(\"select * from joined where overallScore <> 0.0\")\n",
    "val nonZeroCnt = nonZeroScores.count()\n",
    "val zeroScores = spark.sql(\"select * from joined where overallScore = 0.0\")\n",
    "val zeroCount = zeroScores.count()\n",
    "println(s\"folks with QP status having zero scores: ${zeroCount} and folks with erratic (non zero) score count: ${nonZeroCnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "nonZeroScores.select(\"tin\", \"npi\", \"qp_status\" ,\"overallScore\", \"isEligible\", \"isFinal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "spark.sql(\"select tin, npi, qp_status, overallscore, iseligible, isfinal from joined limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o r \n",
    "val r = spark.sql(\"select details from final_scoring.fs_runs where runid = 24\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "// -- A2154, 0000152161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o rollup\n",
    "val rollup = spark.sql(s\"SELECT * FROM final_scoring.rollup_$RUN_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "rollup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "val rollup = spark.sql(s\"SELECT * FROM final_scoring.rollup_$RUN_ID where apmEntityId = 'A2154' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "rollup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "val rollupParticipants = spark.sql(s\"\"\" SELECT * FROM \n",
    "    final_scoring.rollup_participants_$RUN_ID where apmEntityId = 'A2154' \"\"\")\n",
    "rollupParticipants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
